
import numpy as np
import threading
import pyaudio
import time
import sys
import os
import collections
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from scipy import signal
import queue
import librosa

# -------------------------------
# Load GPT-2 Model and Parameters
# -------------------------------
class GPT2Integration:
    def __init__(self, model_size="small"):
        """
        Initialize the GPT-2 model and prepare for quantum integration
        
        Args:
            model_size: Size of GPT-2 model to load ("small", "medium", or "large")
        """
        print(f"Loading GPT-2 {model_size} model...")
        
        # Map model_size to actual model names
        model_map = {
            "small": "gpt2",          # 124M parameters
            "medium": "gpt2-medium",  # 355M parameters
            "large": "gpt2-large"     # 774M parameters
        }
        
        model_name = model_map.get(model_size, "gpt2")
        
        # Load tokenizer and model
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        
        # Move to GPU if available
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        self.model.eval()  # Set to evaluation mode
        
        # Get model dimensions
        self.embedding_dim = self.model.config.n_embd
        self.vocab_size = self.model.config.vocab_size
        self.n_layers = self.model.config.n_layer
        
        # Extract key model parameters
        self.extract_model_parameters()
        
        # Initialize the "audio context" - a buffer of text derived from audio
        self.audio_context = ""
        self.audio_tokens = []
        self.last_generated_text = ""
        
        # Audio-to-embedding mapping parameters
        self.mel_bins = 80  # Number of mel frequency bins to use
        self.context_length = self.model.config.n_ctx  # Maximum context length for GPT-2
        
        print(f"GPT-2 model loaded successfully on {self.device}")
        print(f"Model parameters: Layers={self.n_layers}, Embedding dim={self.embedding_dim}, Vocab size={self.vocab_size}")
    
    def extract_model_parameters(self):
        """Extract and organize key parameters from the GPT-2 model"""
        # Get embeddings
        self.word_embeddings = self.model.transformer.wte.weight.detach().cpu().numpy()
        
        # Get first layer weights as an example
        # In a full implementation, we would extract all layers
        self.first_layer_weights = {
            'attention': {
                'query': self.model.transformer.h[0].attn.c_attn.weight.detach().cpu().numpy()[:, :self.embedding_dim],
                'key': self.model.transformer.h[0].attn.c_attn.weight.detach().cpu().numpy()[:, self.embedding_dim:2*self.embedding_dim],
                'value': self.model.transformer.h[0].attn.c_attn.weight.detach().cpu().numpy()[:, 2*self.embedding_dim:3*self.embedding_dim]
            },
            'mlp': {
                'c_fc': self.model.transformer.h[0].mlp.c_fc.weight.detach().cpu().numpy(),
                'c_proj': self.model.transformer.h[0].mlp.c_proj.weight.detach().cpu().numpy()
            }
        }
        
        # Convert word embeddings to quantum-compatible format
        self.quantum_embeddings = self.prepare_quantum_embeddings(self.word_embeddings)
    
    def prepare_quantum_embeddings(self, embeddings):
        """
        Convert embeddings to quantum-compatible format
        
        In quantum format, we represent each embedding dimension as a phase angle
        between -π and π, allowing for quantum superposition and interference
        """
        # Normalize the embeddings to have values between -π and π
        embedding_norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        normalized_embeddings = embeddings / (embedding_norms + 1e-10)  # Avoid division by zero
        
        # Scale to range [-π, π]
        quantum_embeddings = normalized_embeddings * np.pi
        
        return quantum_embeddings
    
    def audio_to_embedding(self, audio_data, sr=16000):
        """
        Convert audio data to embeddings that can interface with GPT-2
        
        Args:
            audio_data: 1D array of audio samples
            sr: Sample rate of the audio
            
        Returns:
            embedding: Representation that can be used with GPT-2
        """
        # Extract spectral features (mel-spectrogram)
        mel_spec = librosa.feature.melspectrogram(
            y=audio_data.astype(np.float64), 
            sr=sr, 
            n_mels=self.mel_bins,
            hop_length=512
        )
        
        # Convert to log scale (dB)
        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        # Normalize to [0, 1]
        normalized_spec = (log_mel_spec - log_mel_spec.min()) / (log_mel_spec.max() - log_mel_spec.min() + 1e-10)
        
        # Reshape to match embedding dimension expectations
        # We'll average across frequency bins if necessary
        if normalized_spec.shape[1] > self.embedding_dim:
            # Too many time frames, need to downsample
            time_frames = signal.resample(normalized_spec, self.embedding_dim, axis=1)
            embedding = np.mean(time_frames, axis=0)
        else:
            # Pad with zeros if necessary
            embedding = np.mean(normalized_spec, axis=0)
            if len(embedding) < self.embedding_dim:
                embedding = np.pad(embedding, (0, self.embedding_dim - len(embedding)))
            else:
                embedding = embedding[:self.embedding_dim]
        
        # Find closest word embedding via cosine similarity
        # This maps the audio features to the GPT-2 embedding space
        similarity = np.dot(self.quantum_embeddings, embedding) / (
            np.linalg.norm(self.quantum_embeddings, axis=1) * np.linalg.norm(embedding) + 1e-10
        )
        
        # Get the top 5 most similar tokens
        top_indices = np.argsort(similarity)[-5:]
        
        # Return the token indices and their embeddings
        return {
            'token_indices': top_indices,
            'embedding': embedding,
            'confidence': similarity[top_indices]
        }
    
    def decode_tokens_to_text(self, tokens):
        """Convert token IDs to text"""
        return self.tokenizer.decode(tokens)
    
    def generate_from_audio_context(self, max_length=50):
        """Generate text continuation based on audio context"""
        if not self.audio_tokens:
            return "Listening..."
        
        with torch.no_grad():
            # Convert audio tokens to tensor
            input_ids = torch.tensor([self.audio_tokens], dtype=torch.long).to(self.device)
            
            # Generate continuation
            output = self.model.generate(
                input_ids,
                max_length=max_length,
                num_return_sequences=1,
                pad_token_id=self.tokenizer.eos_token_id,
                do_sample=True,
                temperature=0.7,
                top_p=0.92
            )
            
            # Decode the output
            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)
            self.last_generated_text = generated_text
            
            return generated_text
    
    def update_audio_context(self, audio_data, sr=16000):
        """Update the audio context with new audio data"""
        # Convert audio to embedding
        embedding_data = self.audio_to_embedding(audio_data, sr)
        
        # Add the most confident token to our context
        most_confident_token = embedding_data['token_indices'][-1]
        self.audio_tokens.append(int(most_confident_token))
        
        # Keep context to a manageable size
        if len(self.audio_tokens) > self.context_length:
            self.audio_tokens = self.audio_tokens[-self.context_length:]
        
        # Update text context
        self.audio_context = self.decode_tokens_to_text(self.audio_tokens)
        
        return embedding_data
    
    def embedding_to_audio(self, embedding, audio_length=1024):
        """
        Convert GPT-2 embedding back to audio signal
        This is a simple inverse mapping for demonstration
        """
        # Normalize embedding to [-1, 1] range for audio
        normalized_embedding = 2.0 * (embedding - np.min(embedding)) / (np.max(embedding) - np.min(embedding) + 1e-10) - 1.0
        
        # Expand to desired audio length
        # This is a very simplified approach - in practice you'd want a more sophisticated method
        expanded = signal.resample(normalized_embedding, audio_length)
        
        # Apply a window function to smooth the audio
        window = signal.windows.hann(audio_length)
        result = expanded * window
        
        return result

    def text_to_audio(self, text, audio_length=1024):
        """Convert text to audio representation via embeddings"""
        # Tokenize the text
        tokens = self.tokenizer.encode(text, return_tensors="pt").to(self.device)
        
        # Get embeddings from the model
        with torch.no_grad():
            outputs = self.model(tokens, output_hidden_states=True)
            # Get the last hidden state
            embeddings = outputs.hidden_states[-1].detach().cpu().numpy()[0]
        
        # Convert each token's embedding to audio and concatenate
        audio_chunks = []
        for embedding in embeddings:
            audio_chunk = self.embedding_to_audio(embedding, audio_length)
            audio_chunks.append(audio_chunk)
        
        # Concatenate with overlap
        if len(audio_chunks) > 1:
            overlap = audio_length // 4
            result_length = audio_length + (len(audio_chunks) - 1) * (audio_length - overlap)
            result = np.zeros(result_length)
            
            position = 0
            for chunk in audio_chunks:
                result[position:position + audio_length] += chunk
                position += audio_length - overlap
                
            # Normalize
            result = result / np.max(np.abs(result) + 1e-10)
        elif len(audio_chunks) == 1:
            result = audio_chunks[0]
        else:
            result = np.zeros(audio_length)
        
        return result

# -------------------------------
# Quantum Invariant Measure Functions for Audio (r = i)
# -------------------------------
def process_quantum_invariant_1d(U, psi, dx=1.0):
    """
    Process a 1D function U (e.g., audio signal) via the quantum invariant measure framework.
    Uses r = i (imaginary unit) to create quantum wave function behavior.
    Returns the decoded function U_decoded with EXACT same length as input.
    """
    input_length = len(U)
    
    # Compute the quantum wave function: exp(-i*U)
    # This replaces the classical exp(2*U/r) with r=1.0
    wave_function = np.exp(-1j * U)
    
    # Compute normalization in complex domain
    integral_wave = np.sum(np.abs(wave_function)**2 * dx)
    n_complex = (1j / 2) * np.log(integral_wave)
    
    # The probability density follows the Born rule: P = |ψ|²
    # with the normalized wave function: ψ = exp(-i*U) / sqrt(integral)
    normalized_wave = wave_function / np.sqrt(integral_wave)
    P = np.abs(normalized_wave)**2
    
    # Recover U using the inverse transformation
    # This is the quantum analogue of: U = (r/2)*log(P/|ψ|²) + n
    phase = np.angle(normalized_wave * psi.conjugate())
    U_decoded = -phase  # The negative of the phase recovers the original U
    
    # Ensure output length matches input length exactly
    assert len(U_decoded) == input_length, f"Length mismatch: input {input_length}, output {len(U_decoded)}"
    return U_decoded

def create_quantum_wavefunction_1d(length, sigma_factor=0.5):
    """Create a normalized 1D complex Gaussian wavefunction for a given length."""
    center = (length - 1) / 2.0
    sigma = sigma_factor * (length / 2.0)
    x = np.arange(length)
    sq_dist = ((x - center) ** 2) / (2 * sigma**2)
    
    # Create complex wavefunction with phase = 0
    psi = np.exp(-sq_dist)
    norm = np.sqrt(np.sum(np.abs(psi)**2))
    return psi / norm

# -------------------------------
# Audio Processing with GPT-2 Integration
# -------------------------------
class QuantumAudioGPT2Buffer:
    """Buffer for managing quantum audio with GPT-2 integration"""
    def __init__(self, max_size=10, chunk_size=1024):
        self.buffer = collections.deque(maxlen=max_size)
        self.chunk_size = chunk_size
        self.feedback_level = 0.3  # Default feedback level (0.0 to 1.0)
        self.mic_level = 0.7       # Default microphone level (0.0 to 1.0)
        self.prediction_level = 0.5  # Default prediction level (0.0 to 1.0)
        self.quantum_interference = 0.5  # Quantum interference level (0.0 to 1.0)
        self.gpt2_influence = 0.5  # GPT-2 influence level (0.0 to 1.0)
        
        # Text-to-speech buffer
        self.tts_queue = queue.Queue()
        
    def add_output(self, audio_array):
        """Add processed output to the buffer"""
        # Make a copy to avoid reference issues
        self.buffer.append(audio_array.copy())
        
    def get_feedback(self):
        """Get mixed feedback from the buffer with quantum interference"""
        if not self.buffer:
            return np.zeros(self.chunk_size, dtype=np.float64)
        
        # Create a complex feedback array for quantum interference
        feedback = np.zeros(self.chunk_size, dtype=np.complex128)
        count = 0
        
        # Perform quantum superposition by adding complex amplitudes
        for sample in self.buffer:
            # Convert to complex amplitude (phase encoding)
            complex_sample = np.exp(1j * sample)
            feedback += complex_sample
            count += 1
            
        if count > 0:
            # Normalize the superposition
            feedback /= np.sqrt(count)
            
        # Convert back to real audio using Born rule
        # Mix between quantum (interference) and classical (average) based on quantum_interference level
        quantum_feedback = np.angle(feedback)
        
        # Get classical feedback (average of real samples)
        classical_feedback = np.zeros(self.chunk_size, dtype=np.float64)
        for sample in self.buffer:
            classical_feedback += sample
        classical_feedback /= max(1, len(self.buffer))
        
        # Mix quantum and classical feedback
        mixed_feedback = (self.quantum_interference * quantum_feedback + 
                         (1 - self.quantum_interference) * classical_feedback)
        
        return mixed_feedback
    
    def mix_with_input(self, input_array):
        """Mix the feedback with new input using current levels"""
        feedback = self.get_feedback()
        
        # Scale each component by its level and ensure they sum to 1.0
        total = self.mic_level + self.feedback_level
        if total == 0:
            # Avoid division by zero
            normalized_mic = 1.0
            normalized_feedback = 0.0
        else:
            normalized_mic = self.mic_level / total
            normalized_feedback = self.feedback_level / total
        
        # Mix input with feedback, maintaining overall amplitude
        result = (normalized_mic * input_array) + (normalized_feedback * feedback)
        
        return result
    
    def mix_with_gpt2(self, audio_signal, gpt2_audio):
        """Mix processed audio with GPT-2 generated audio"""
        # Convert audio signals to complex amplitudes
        audio_complex = np.exp(1j * audio_signal)
        gpt2_complex = np.exp(1j * gpt2_audio)
        
        # Quantum superposition with weights
        superposition = np.sqrt(1.0 - self.gpt2_influence) * audio_complex + \
                        np.sqrt(self.gpt2_influence) * gpt2_complex
        
        # Extract phase (angle) for the resulting audio
        result = np.angle(superposition)
        
        return result
    
    def mix_with_prediction(self, processed, predicted):
        """Mix processed audio with its prediction using quantum superposition"""
        # Create complex amplitudes for quantum mixing
        processed_complex = np.exp(1j * processed)
        predicted_complex = np.exp(1j * predicted)
        
        # Quantum superposition with relative weights
        superposition = np.sqrt(1.0 - self.prediction_level) * processed_complex + \
                        np.sqrt(self.prediction_level) * predicted_complex
        
        # Extract phase (angle) for the resulting audio
        result = np.angle(superposition)
        
        return result
    
    def add_text_for_speech(self, text):
        """Add text to be converted to audio"""
        self.tts_queue.put(text)
    
    def get_text_for_speech(self):
        """Get the next piece of text to be converted to audio"""
        if self.tts_queue.empty():
            return None
        return self.tts_queue.get()
    
    def adjust_levels(self, feedback_level=None, mic_level=None, prediction_level=None, 
                      quantum_level=None, gpt2_level=None):
        """Adjust the feedback, microphone, prediction, quantum and GPT-2 levels"""
        if feedback_level is not None:
            self.feedback_level = max(0.0, min(1.0, feedback_level))
        if mic_level is not None:
            self.mic_level = max(0.0, min(1.0, mic_level))
        if prediction_level is not None:
            self.prediction_level = max(0.0, min(1.0, prediction_level))
        if quantum_level is not None:
            self.quantum_interference = max(0.0, min(1.0, quantum_level))
        if gpt2_level is not None:
            self.gpt2_influence = max(0.0, min(1.0, gpt2_level))
            
        print(f"Levels - Mic: {self.mic_level:.2f}, Feedback: {self.feedback_level:.2f}, "
              f"Prediction: {self.prediction_level:.2f}, Quantum: {self.quantum_interference:.2f}, "
              f"GPT2: {self.gpt2_influence:.2f}")

# -------------------------------
# Global variables for coordination
# -------------------------------
global_last_audio = None
running = True
feedback_buffer = None  # Will be initialized in audio thread
gpt2_system = None  # Will be initialized in main

# -------------------------------
# GPT-2 Text Generation Thread 
# -------------------------------
def gpt2_thread():
    """Thread that handles GPT-2 text generation from audio"""
    global running, gpt2_system, feedback_buffer
    
    # Wait for initialization of other components
    while running and (gpt2_system is None or feedback_buffer is None):
        time.sleep(0.5)
    
    if not running:
        return
    
    print("GPT-2 text generation thread started")
    
    # Buffer to collect audio for processing
    audio_collection_buffer = []
    target_collection_size = 16000  # 1 second at 16kHz
    
    # Timer for periodic text generation
    last_generation_time = time.time()
    generation_interval = 5.0  # Generate text every 5 seconds
    
    while running:
        try:
            # Check if it's time to generate text
            current_time = time.time()
            if current_time - last_generation_time >= generation_interval and audio_collection_buffer:
                # Concatenate collected audio
                audio_segment = np.concatenate(audio_collection_buffer)
                
                # Update GPT-2 context with the audio
                embedding_data = gpt2_system.update_audio_context(audio_segment)
                
                # Generate text based on the updated context
                generated_text = gpt2_system.generate_from_audio_context(max_length=50)
                print(f"\nGenerated from audio context: {generated_text}")
                
                # Add text for speech synthesis
                feedback_buffer.add_text_for_speech(generated_text)
                
                # Reset collection buffer and update timer
                audio_collection_buffer = []
                last_generation_time = current_time
            
            # Sleep to prevent tight loop
            time.sleep(0.1)
            
        except Exception as e:
            print(f"Error in GPT-2 thread: {e}")
            import traceback
            traceback.print_exc()
            time.sleep(1.0)  # Longer sleep on error

# -------------------------------
# Audio Processing Thread
# -------------------------------
def audio_thread():
    global global_last_audio, running, feedback_buffer, gpt2_system
    
    p = pyaudio.PyAudio()
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000
    CHUNK = 1024
    
    # Try to open audio streams with retry logic
    stream_in = None
    stream_out = None
    
    # List available audio devices
    print("\nAvailable audio input devices:")
    for i in range(p.get_device_count()):
        dev_info = p.get_device_info_by_index(i)
        if dev_info['maxInputChannels'] > 0:
            print(f"  Input Device {i}: {dev_info['name']}")
    
    print("\nAvailable audio output devices:")
    for i in range(p.get_device_count()):
        dev_info = p.get_device_info_by_index(i)
        if dev_info['maxOutputChannels'] > 0:
            print(f"  Output Device {i}: {dev_info['name']}")
    
    # Find default devices
    default_input = p.get_default_input_device_info()['index'] if p.get_default_input_device_info() else 0
    default_output = p.get_default_output_device_info()['index'] if p.get_default_output_device_info() else 0
    
    print(f"\nUsing default input device: {default_input}")
    print(f"Using default output device: {default_output}")
    
    # Try to open audio streams
    max_retries = 5
    
    for retry in range(max_retries):
        try:
            print(f"Attempt {retry+1}/{max_retries} to open audio streams...")
            stream_in = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,
                              input=True, frames_per_buffer=CHUNK,
                              input_device_index=default_input)
            stream_out = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,
                               output=True, frames_per_buffer=CHUNK,
                               output_device_index=default_output)
            break
        except Exception as e:
            print(f"Error opening audio streams: {e}")
            # Clean up any partially opened streams
            if stream_in:
                try:
                    stream_in.close()
                except:
                    pass
                stream_in = None
            if stream_out:
                try:
                    stream_out.close()
                except:
                    pass
                stream_out = None
            
            if retry == max_retries - 1:
                print("Failed to open audio streams after multiple attempts")
                running = False
                return
            
            # Wait before retry
            wait_time = 2 * (retry + 1)
            print(f"Waiting {wait_time} seconds before retry...")
            time.sleep(wait_time)
    
    # Create quantum feedback buffer with GPT-2 integration
    feedback_buffer = QuantumAudioGPT2Buffer(max_size=5, chunk_size=CHUNK)
    
    print("\nQuantum Audio Processing with GPT-2 integration started")
    print(f"Processing audio chunks of exactly {CHUNK} samples at {RATE}Hz")
    print(f"Initial settings:")
    print(f"  - Mic level: {feedback_buffer.mic_level:.2f}")
    print(f"  - Feedback level: {feedback_buffer.feedback_level:.2f}")
    print(f"  - Prediction level: {feedback_buffer.prediction_level:.2f}")
    print(f"  - Quantum interference: {feedback_buffer.quantum_interference:.2f}")
    print(f"  - GPT-2 influence: {feedback_buffer.gpt2_influence:.2f}")
    print("Controls:")
    print("  - Press Ctrl+C to stop")
    print("  - Press + or - to adjust feedback level up/down")
    print("  - Press m or n to adjust microphone level up/down")
    print("  - Press p or o to adjust prediction level up/down")
    print("  - Press q or w to adjust quantum interference up/down")
    print("  - Press g or h to adjust GPT-2 influence up/down")
    
    # Create a template quantum wavefunction for the chunk size
    template_psi = create_quantum_wavefunction_1d(CHUNK, sigma_factor=0.5)
    
    # Stats tracking
    processed_chunks = 0
    start_time = time.time()
    last_report_time = start_time
    
    # Audio buffer for GPT-2 processing
    gpt2_audio_buffer = []
    
    while running:
        try:
            # Read exact CHUNK size from microphone
            data = stream_in.read(CHUNK, exception_on_overflow=False)
            mic_audio = np.frombuffer(data, dtype=np.int16).astype(np.float64) / 32768.0
            
            # Verify input size
            assert len(mic_audio) == CHUNK, f"Input audio chunk size mismatch: {len(mic_audio)} vs {CHUNK}"
            
            # Add raw audio to GPT-2 buffer for processing in the other thread
            gpt2_audio_buffer.append(mic_audio.copy())
            if len(gpt2_audio_buffer) * CHUNK > RATE:  # More than 1 second
                # Send to GPT-2 system
                if gpt2_system:
                    try:
                        # Flatten the buffer
                        audio_segment = np.concatenate(gpt2_audio_buffer)
                        # Update audio context in a non-blocking way
                        gpt2_system.update_audio_context(audio_segment)
                    except Exception as e:
                        print(f"Error updating GPT-2 context: {e}")
                # Reset buffer
                gpt2_audio_buffer = []
            
            # Check if there's text to convert to audio
            gpt2_audio = None
            text_for_speech = feedback_buffer.get_text_for_speech()
            if text_for_speech and gpt2_system:
                try:
                    # Convert text to audio representation
                    gpt2_audio = gpt2_system.text_to_audio(text_for_speech, CHUNK)
                    print(f"Converting to audio: '{text_for_speech[:50]}...'")
                except Exception as e:
                    print(f"Error converting text to audio: {e}")
            
            # Mix input with feedback
            mixed_audio = feedback_buffer.mix_with_input(mic_audio)
            
            # Process mixed audio using quantum invariant measure (r = i)
            audio_decoded = process_quantum_invariant_1d(mixed_audio, template_psi, dx=1.0)
            
            # Verify processed size
            assert len(audio_decoded) == CHUNK, f"Processed audio size mismatch: {len(audio_decoded)} vs {CHUNK}"
            
            # Quantum prediction: using phase-space approach
            if global_last_audio is None or len(global_last_audio) != len(audio_decoded):
                audio_predicted = audio_decoded.copy()
            else:
                # Create wave functions from current and previous frames
                psi_current = np.exp(1j * audio_decoded)
                psi_previous = np.exp(1j * global_last_audio)
                
                # Compute phase difference (momentum in phase space)
                phase_diff = np.angle(psi_current / psi_previous)
                
                # Extrapolate using quantum "momentum"
                psi_predicted = psi_current * np.exp(1j * phase_diff)
                
                # Extract phase as the predicted audio
                audio_predicted = np.angle(psi_predicted)
                
            # Store current processed frame for next iteration's prediction
            global_last_audio = audio_decoded.copy()
            
            # Mix processed audio with prediction based on prediction level
            predicted_output = feedback_buffer.mix_with_prediction(audio_decoded, audio_predicted)
            
            # Mix with GPT-2 audio if available
            if gpt2_audio is not None:
                final_output = feedback_buffer.mix_with_gpt2(predicted_output, gpt2_audio)
            else:
                final_output = predicted_output
            
            # Add final output to feedback buffer for future iterations
            feedback_buffer.add_output(final_output)
            
            # Prepare output (normalize to avoid distortion)
            # Scale audio to fit within [-1, 1] range
            max_val = np.max(np.abs(final_output))
            if max_val > 1e-10:  # Avoid division by zero
                audio_out = final_output / max_val
            else:
                audio_out = final_output
                
            # Clip to prevent any overflows
            audio_out = np.clip(audio_out, -1.0, 1.0)
            
            # Convert to bytes for output
            audio_out_bytes = (audio_out * 32767).astype(np.int16).tobytes()
            
            # Verify output length in bytes
            expected_bytes = CHUNK * 2  # 16-bit = 2 bytes per sample
            assert len(audio_out_bytes) == expected_bytes, f"Output audio byte size mismatch: {len(audio_out_bytes)} vs {expected_bytes}"
            
            # Send to speaker
            stream_out.write(audio_out_bytes)
            
            # Update stats
            processed_chunks += 1
            current_time = time.time()
            elapsed = current_time - last_report_time
            
            # Print stats every 5 seconds
            if elapsed >= 5.0:
                chunks_per_second = processed_chunks / (current_time - start_time)
                print(f"Stats: Processed {processed_chunks} audio chunks ({chunks_per_second:.1f} chunks/sec)")
                last_report_time = current_time
                
        except Exception as e:
            print(f"Audio processing error: {e}")
            import traceback
            traceback.print_exc()
            time.sleep(0.1)  # Prevent tight loop on errors
    
    # Clean up
    try:
        if stream_in:
            stream_in.stop_stream()
            stream_in.close()
        if stream_out:
            stream_out.stop_stream()
            stream_out.close()
        p.terminate()
    except Exception as e:
        print(f"Error during audio cleanup: {e}")
    
    print("Quantum audio processing terminated.")

# -------------------------------
# Keyboard Control Thread
# -------------------------------
def keyboard_control_thread():
    """Thread to handle keyboard controls for adjustment"""
    global running, feedback_buffer
    
    # Wait for audio thread to create the feedback buffer
    while running and feedback_buffer is None:
        time.sleep(0.1)
    
    if not running:
        return
    
    print("Keyboard controls active.")
    
    if sys.platform == 'win32':
        import msvcrt
        
        while running:
            if msvcrt.kbhit():
                key = msvcrt.getch().decode('utf-8', errors='ignore')
                if key == '+':
                    feedback_buffer.adjust_levels(feedback_level=feedback_buffer.feedback_level + 0.1)
                elif key == '-':
                    feedback_buffer.adjust_levels(feedback_level=feedback_buffer.feedback_level - 0.1)
                elif key.lower() == 'm':
                    feedback_buffer.adjust_levels(mic_level=feedback_buffer.mic_level + 0.1)
                elif key.lower() == 'n':
                    feedback_buffer.adjust_levels(mic_level=feedback_buffer.mic_level - 0.1)
                elif key.lower() == 'p':
                    feedback_buffer.adjust_levels(prediction_level=feedback_buffer.prediction_level + 0.1)
                elif key.lower() == 'o':
                    feedback_buffer.adjust_levels(prediction_level=feedback_buffer.prediction_level - 0.1)
                elif key.lower() == 'q':
                    feedback_buffer.adjust_levels(quantum_level=feedback_buffer.quantum_interference + 0.1)
                elif key.lower() == 'w':
                    feedback_buffer.adjust_levels(quantum_level=feedback_buffer.quantum_interference - 0.1)
                elif key.lower() == 'g':
                    feedback_buffer.adjust_levels(gpt2_level=feedback_buffer.gpt2_influence + 0.1)
                elif key.lower() == 'h':
                    feedback_buffer.adjust_levels(gpt2_level=feedback_buffer.gpt2_influence - 0.1)
            time.sleep(0.1)
    else:
        # Unix-like systems
        import termios, tty, select
        
        old_settings = termios.tcgetattr(sys.stdin)
        try:
            tty.setcbreak(sys.stdin.fileno())
            
            while running:
                if select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], []):
                    key = sys.stdin.read(1)
                    if key == '+':
                        feedback_buffer.adjust_levels(feedback_level=feedback_buffer.feedback_level + 0.1)
                    elif key == '-':
                        feedback_buffer.adjust_levels(feedback_level=feedback_buffer.feedback_level - 0.1)
                    elif key.lower() == 'm':
                        feedback_buffer.adjust_levels(mic_level=feedback_buffer.mic_level + 0.1)
                    elif key.lower() == 'n':
                        feedback_buffer.adjust_levels(mic_level=feedback_buffer.mic_level - 0.1)
                    elif key.lower() == 'p':
                        feedback_buffer.adjust_levels(prediction_level=feedback_buffer.prediction_level + 0.1)
                    elif key.lower() == 'o':
                        feedback_buffer.adjust_levels(prediction_level=feedback_buffer.prediction_level - 0.1)
                    elif key.lower() == 'q':
                        feedback_buffer.adjust_levels(quantum_level=feedback_buffer.quantum_interference + 0.1)
                    elif key.lower() == 'w':
                        feedback_buffer.adjust_levels(quantum_level=feedback_buffer.quantum_interference - 0.1)
                    elif key.lower() == 'g':
                        feedback_buffer.adjust_levels(gpt2_level=feedback_buffer.gpt2_influence + 0.1)
                    elif key.lower() == 'h':
                        feedback_buffer.adjust_levels(gpt2_level=feedback_buffer.gpt2_influence - 0.1)
                time.sleep(0.1)
        finally:
            termios.tcsetattr(sys.stdin, termios.TCSADRAIN, old_settings)

# =============================================================================
# Main: Start All Threads
# =============================================================================
if __name__ == "__main__":
    try:
        print("\n=== Quantum Audio Processor with GPT-2 Integration ===")
        print("This system processes audio using quantum principles (r = i)")
        print("and integrates with GPT-2 language model for audio-text synergy")
        
        print("\nInitializing GPT-2 model (this may take a moment)...")
        gpt2_system = GPT2Integration(model_size="small")  # Options: "small", "medium", "large"
        
        print("Starting audio processing...")
        
        # Create and start audio thread
        aud_thread = threading.Thread(target=audio_thread, name="AudioThread")
        aud_thread.start()
        
        # Create and start GPT-2 thread
        gpt2_thread = threading.Thread(target=gpt2_thread, name="GPT2Thread")
        gpt2_thread.daemon = True  # This thread will exit when main thread exits
        gpt2_thread.start()
        
        # Create keyboard control thread
        keyboard_thread = threading.Thread(target=keyboard_control_thread, name="KeyboardThread")
        keyboard_thread.daemon = True  # This thread will exit when main thread exits
        keyboard_thread.start()
        
        # Wait for keyboard interrupt in main thread
        try:
            while running and aud_thread.is_alive():
                time.sleep(0.1)
        except KeyboardInterrupt:
            print("\nKeyboard interrupt detected. Shutting down...")
            running = False
        
        # Wait for audio thread to finish
        if aud_thread.is_alive():
            aud_thread.join(timeout=2.0)
        
    except Exception as e:
        print(f"Error in main thread: {e}")
        running = False
    finally:
        print("\nProgram terminated.")
