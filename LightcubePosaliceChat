import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import savgol_filter
from scipy.fft import fft, ifft, fftfreq
import time
import re
import os
import pickle
from datetime import datetime

class PosaliceLanguageModel:
    """
    A phase-based language model that learns from text input using the Posalice approach.
    Represents language as functions in phase space and adapts in real-time.
    Uses unlimited context and supports massive output generation.
    """
    
    def __init__(self, vocab_size=10000, embedding_dim=128, filter_param=0.3):
        """
        Initialize the Posalice Language Model with unlimited context.
        
        Parameters:
        -----------
        vocab_size : int
            Maximum vocabulary size
        embedding_dim : int
            Dimensionality of text embeddings
        filter_param : float
            Phase filtering parameter (0-1)
        """
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.filter_param = filter_param
        
        # Initialize phase spaces BEFORE adding words
        self.token_phases = np.zeros((vocab_size, embedding_dim))
        self.context_phases = np.zeros((vocab_size, embedding_dim))
        
        # Initialize vocabulary
        self.word_to_idx = {}
        self.idx_to_word = {}
        self.word_count = {}
        self.vocab_count = 0
        self.next_idx = 0
        
        # Token frequency tracking
        self.token_frequencies = np.ones(vocab_size)  # Laplace smoothing
        
        # Add special tokens
        self._add_word("<PAD>")  # Padding token
        self._add_word("<UNK>")  # Unknown token
        self._add_word("<START>")  # Sequence start
        self._add_word("<END>")  # Sequence end
        
        # Learning rate and decay
        self.learn_rate = 0.1
        self.learn_decay = 0.9995
        self.min_learn_rate = 0.001
        
        # Full conversation history with unbounded context
        self.conversation_history = []
        self.max_history_tokens = 100000  # Effectively unlimited for practical purposes
        
        # Tracking for adaptation
        self.last_train_time = time.time()
        self.messages_since_update = 0
        
        # Phase-space cache for faster response generation
        self.phase_cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        print(f"Initialized Posalice Language Model:")
        print(f"  Vocabulary size: {vocab_size}")
        print(f"  Embedding dimensions: {embedding_dim}")
        print(f"  Unlimited context with {self.max_history_tokens} token history")
        print(f"  Learning rate: {self.learn_rate}")
    
    def _add_word(self, word):
        """Add a word to the vocabulary if not present."""
        if word not in self.word_to_idx and self.vocab_count < self.vocab_size:
            idx = self.next_idx
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
            self.word_count[word] = 1
            self.vocab_count += 1
            self.next_idx += 1
            
            # Initialize with random phase for this token
            self.token_phases[idx] = np.random.uniform(-np.pi, np.pi, self.embedding_dim)
            return idx
        elif word in self.word_to_idx:
            self.word_count[word] += 1
            return self.word_to_idx[word]
        else:
            # Vocabulary is full, return unknown token
            return self.word_to_idx["<UNK>"]
    
    def _tokenize(self, text):
        """
        Basic tokenization of text into words.
        
        Parameters:
        -----------
        text : str
            Text to tokenize
            
        Returns:
        --------
        tokens : list
            List of token indices
        """
        # Simple tokenization (split by whitespace and punctuation)
        words = re.findall(r'\b\w+\b|[.,!?;:"]', text.lower())
        
        # Convert to token indices, add unknown words to vocabulary
        tokens = []
        for word in words:
            if word in self.word_to_idx:
                idx = self.word_to_idx[word]
            elif self.vocab_count < self.vocab_size:
                idx = self._add_word(word)
            else:
                idx = self.word_to_idx["<UNK>"]
            
            tokens.append(idx)
            
            # Update token frequency
            self.token_frequencies[idx] += 1
        
        return tokens
    
    def _phase_encode(self, indices, phase_array):
        """
        Encode token indices as phase vectors.
        
        Parameters:
        -----------
        indices : list
            List of token indices
        phase_array : array
            Phase array to use for encoding (token_phases or context_phases)
            
        Returns:
        --------
        phases : array
            Phase encoding of tokens (shape: [len(indices), embedding_dim])
        """
        return np.array([phase_array[idx] for idx in indices])
    
    def _phase_filter(self, phases):
        """
        Apply filtering to phase representation.
        
        Parameters:
        -----------
        phases : array
            Phase values
            
        Returns:
        --------
        filtered_phases : array
            Filtered phase values
        """
        # Determine filter window size
        window_size = max(5, int(len(phases) * self.filter_param * 0.2))
        if window_size % 2 == 0:
            window_size += 1
            
        # Apply filtering along each dimension
        filtered_phases = np.copy(phases)
        for i in range(phases.shape[1]):
            filtered_phases[:, i] = savgol_filter(
                phases[:, i], 
                window_size, 
                2, 
                mode='nearest'
            )
        
        return filtered_phases
    
    def _complex_representation(self, phases):
        """
        Convert phases to complex representation.
        
        Parameters:
        -----------
        phases : array
            Phase values
            
        Returns:
        --------
        complex_rep : array
            Complex representation
        """
        return np.exp(1j * phases)
    
    def _phase_similarity(self, phase_a, phase_b):
        """
        Calculate similarity between phase representations.
        
        Parameters:
        -----------
        phase_a, phase_b : array
            Phase representations
            
        Returns:
        --------
        similarity : float
            Similarity score (0-1)
        """
        # Convert to complex representation
        complex_a = np.exp(1j * phase_a)
        complex_b = np.exp(1j * phase_b)
        
        # Calculate dot product and normalize
        dot_product = np.abs(np.sum(complex_a * np.conj(complex_b))) / len(phase_a)
        
        return dot_product
    
    def _get_full_context_phase(self, tokens, max_tokens=None):
        """
        Calculate context phase representation from all available tokens.
        Uses exponential decay weighting to give more importance to recent tokens.
        
        Parameters:
        -----------
        tokens : list
            List of token indices
        max_tokens : int or None
            Maximum number of tokens to consider (None for all)
            
        Returns:
        --------
        context_phase : array
            Phase representation of context
        """
        # Use all tokens if max_tokens is None, otherwise use the most recent ones
        if max_tokens is not None and len(tokens) > max_tokens:
            context_tokens = tokens[-max_tokens:]
        else:
            context_tokens = tokens
        
        if len(context_tokens) == 0:
            # Return zero phase if no context
            return np.zeros(self.embedding_dim)
        
        # Generate a cache key for this context
        cache_key = tuple(context_tokens[-50:])  # Use last 50 tokens to limit key size
        
        # Check if we have this in cache
        if cache_key in self.phase_cache:
            self.cache_hits += 1
            return self.phase_cache[cache_key]
        
        self.cache_misses += 1
        
        # Get phase representation
        token_phases = self._phase_encode(context_tokens, self.token_phases)
        
        # Apply exponential decay weighting based on position
        # More recent tokens get higher weights
        positions = np.arange(len(context_tokens))
        decay_factor = 0.98  # Controls how quickly influence decays with distance
        weights = decay_factor ** (len(context_tokens) - 1 - positions)
        
        # Apply weights to phases
        weighted_phases = token_phases * weights[:, np.newaxis]
        
        # Combine phases (using circular mean)
        complex_rep = np.exp(1j * weighted_phases)
        mean_complex = np.sum(complex_rep, axis=0) / np.sum(weights)
        context_phase = np.angle(mean_complex)
        
        # Cache the result
        if len(self.phase_cache) > 10000:  # Limit cache size
            self.phase_cache.clear()
        self.phase_cache[cache_key] = context_phase
        
        return context_phase
    
    def _update_phases(self, context, next_token, target_phase):
        """
        Update phase representations based on observed sequence.
        
        Parameters:
        -----------
        context : list
            Context token indices
        next_token : int
            Next token index
        target_phase : array
            Target phase for next token
        """
        # Calculate current prediction error
        current_phase = self.context_phases[next_token]
        phase_error = np.angle(np.exp(1j * (target_phase - current_phase)))
        
        # Update next token phase
        self.context_phases[next_token] += self.learn_rate * phase_error
        
        # Update token phases for context tokens
        # Use exponential decay for context token importance
        for i, token in enumerate(context):
            # Weight updates by recency (more recent tokens get larger updates)
            recency_weight = 0.95 ** (len(context) - i - 1)
            update = self.learn_rate * recency_weight * phase_error
            self.token_phases[token] += update
        
        # Decay learning rate
        self.learn_rate = max(self.min_learn_rate, self.learn_rate * self.learn_decay)
    
    def train_on_text(self, text):
        """
        Train the model on a piece of text.
        
        Parameters:
        -----------
        text : str
            Text to train on
        """
        # Tokenize the text
        tokens = self._tokenize(text)
        
        # Add to conversation history
        self.conversation_history.extend(tokens)
        
        # Limit history size
        if len(self.conversation_history) > self.max_history_tokens:
            self.conversation_history = self.conversation_history[-self.max_history_tokens:]
        
        # Process each token in sequence
        for i in range(1, len(tokens)):
            # Get all previous tokens as context
            context = self.conversation_history[:-1]  # All except current token
            
            # Get current token
            next_token = tokens[i]
            
            # Calculate context phase using all available history
            context_phase = self._get_full_context_phase(context)
            
            # Update phase representations
            target_phase = self.token_phases[next_token]
            self._update_phases(context[-100:], next_token, target_phase)  # Use last 100 tokens for updating
        
        self.messages_since_update += 1
        self.last_train_time = time.time()
        
        # Clear phase cache
        self.phase_cache.clear()
    
    def _score_next_tokens(self, context_phase, top_k=50):
        """
        Score possible next tokens based on context phase.
        
        Parameters:
        -----------
        context_phase : array
            Phase representation of context
        top_k : int
            Number of top candidates to return
            
        Returns:
        --------
        candidates : list
            List of (token_idx, score) tuples
        """
        scores = []
        
        # Fast vectorized calculation of similarities
        # Convert all token phases to complex form
        all_complex = np.exp(1j * self.context_phases[:self.vocab_count])
        context_complex = np.exp(1j * context_phase)
        
        # Calculate dot products
        similarities = np.abs(np.sum(all_complex * np.conj(context_complex), axis=1)) / self.embedding_dim
        
        # Apply frequency-based smoothing
        freq_factors = np.log(1 + self.token_frequencies[:self.vocab_count]) / np.log(1 + np.max(self.token_frequencies))
        smoothed_scores = similarities * (0.5 + 0.5 * freq_factors)
        
        # Create (idx, score) pairs
        for i in range(self.vocab_count):
            # Skip special tokens for generation
            if i < 4:  # Skip PAD, UNK, START, END
                continue
            
            scores.append((i, smoothed_scores[i]))
        
        # Sort by score and take top k
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_k]
    
    def _generate_next_token(self, context_tokens, temperature=0.7):
        """
        Generate the next token given context tokens.
        
        Parameters:
        -----------
        context_tokens : list
            List of context token indices
        temperature : float
            Temperature for controlling randomness (lower = more deterministic)
            
        Returns:
        --------
        next_token : int
            Index of next token
        """
        # Get context phase using all available history
        context_phase = self._get_full_context_phase(context_tokens)
        
        # Score candidate tokens
        candidates = self._score_next_tokens(context_phase, top_k=100)
        
        # Apply temperature to scores
        scores = np.array([score for _, score in candidates])
        scores = scores / temperature
        scores = np.exp(scores) / np.sum(np.exp(scores))  # Softmax
        
        # Sample token based on scores
        token_indices = [idx for idx, _ in candidates]
        try:
            next_token = np.random.choice(token_indices, p=scores)
        except:
            # Fallback if probabilities don't sum to 1 (numerical issues)
            next_token = token_indices[0]
        
        return next_token
    
    def generate_text(self, seed_text="", max_length=5000, temperature=0.7):
        """
        Generate text starting from a seed text with support for massive output.
        
        Parameters:
        -----------
        seed_text : str
            Text to start with
        max_length : int
            Maximum number of tokens to generate (can be very large)
        temperature : float
            Temperature for controlling randomness
            
        Returns:
        --------
        generated_text : str
            Generated text
        """
        # Tokenize seed text
        if seed_text:
            seed_tokens = self._tokenize(seed_text)
            # Combine with conversation history for full context
            tokens = self.conversation_history + seed_tokens
        else:
            # Use conversation history plus START token
            tokens = self.conversation_history + [self.word_to_idx["<START>"]]
        
        # Generate tokens
        generated_tokens = []
        for _ in range(max_length):
            # Get next token using full context
            next_token = self._generate_next_token(tokens, temperature)
            
            # Add to generated tokens
            generated_tokens.append(next_token)
            tokens.append(next_token)
            
            # Stop if END token or if we've generated too many tokens
            if next_token == self.word_to_idx["<END>"]:
                break
            
            # Provide progress for long generations
            if len(generated_tokens) % 500 == 0 and len(generated_tokens) > 0:
                print(f"Generated {len(generated_tokens)} tokens so far...")
        
        # Convert tokens to text
        words = [self.idx_to_word[idx] for idx in generated_tokens]
        
        # Process special tokens and join
        processed_words = []
        for word in words:
            if word in ["<PAD>", "<UNK>", "<START>", "<END>"]:
                continue
            processed_words.append(word)
        
        # Basic text reconstruction - this is simplified and could be improved
        text = " ".join(processed_words)
        
        # Fix basic punctuation spacing
        text = re.sub(r' ([.,!?;:"])', r'\1', text)
        
        return text
    
    def respond_to_message(self, message, max_length=5000, temperature=0.7):
        """
        Generate a response to a message and learn from it.
        
        Parameters:
        -----------
        message : str
            User message
        max_length : int
            Maximum response length
        temperature : float
            Temperature for generation
            
        Returns:
        --------
        response : str
            Generated response
        """
        # Train on the message
        self.train_on_text(message)
        
        # Generate response
        response = self.generate_text(seed_text=message[-100:], 
                                     max_length=max_length, 
                                     temperature=temperature)
        
        return response
    
    def adapt_to_conversation(self):
        """
        Adapt model parameters based on conversation progress.
        """
        # Reduce temperature as conversation progresses
        elapsed_time = time.time() - self.last_train_time
        
        # If it's been a while since last message, reset learning rate
        if elapsed_time > 60:  # 1 minute
            self.learn_rate = min(0.1, self.learn_rate * 1.5)
            print(f"Learning rate adjusted to {self.learn_rate:.4f} after pause")
        
        # If we've had many messages, consolidate learning
        if self.messages_since_update > 10:
            self._consolidate_learning()
            self.messages_since_update = 0
            
        # Report cache performance occasionally
        total = self.cache_hits + self.cache_misses
        if total > 0 and total % 100 == 0:
            hit_rate = self.cache_hits / total * 100
            print(f"Phase cache hit rate: {hit_rate:.1f}% ({self.cache_hits}/{total})")
    
    def _consolidate_learning(self):
        """
        Consolidate learning by filtering phase representations.
        """
        # Apply smoothing to token phases
        for i in range(self.vocab_count):
            self.token_phases[i] = savgol_filter(
                self.token_phases[i], 
                min(5, self.embedding_dim), 
                2
            )
            self.context_phases[i] = savgol_filter(
                self.context_phases[i],
                min(5, self.embedding_dim),
                2
            )
        
        print("Consolidated learning through phase filtering")
    
    def save_model(self, filename="posalice_language_model.pkl"):
        """
        Save the model to a file.
        
        Parameters:
        -----------
        filename : str
            Filename to save to
        """
        # Create a dictionary with all the model data
        model_data = {
            'vocab_size': self.vocab_size,
            'embedding_dim': self.embedding_dim,
            'filter_param': self.filter_param,
            'word_to_idx': self.word_to_idx,
            'idx_to_word': self.idx_to_word,
            'word_count': self.word_count,
            'vocab_count': self.vocab_count,
            'next_idx': self.next_idx,
            'token_frequencies': self.token_frequencies,
            'token_phases': self.token_phases,
            'context_phases': self.context_phases,
            'learn_rate': self.learn_rate,
            'conversation_history': self.conversation_history
        }
        
        # Save to file
        with open(filename, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"Model saved to {filename}")
    
    def load_model(self, filename="posalice_language_model.pkl"):
        """
        Load the model from a file.
        
        Parameters:
        -----------
        filename : str
            Filename to load from
        """
        if not os.path.exists(filename):
            print(f"Model file {filename} not found")
            return False
        
        try:
            # Load from file
            with open(filename, 'rb') as f:
                model_data = pickle.load(f)
            
            # Update model attributes
            self.vocab_size = model_data['vocab_size']
            self.embedding_dim = model_data['embedding_dim']
            self.filter_param = model_data['filter_param']
            self.word_to_idx = model_data['word_to_idx']
            self.idx_to_word = model_data['idx_to_word']
            self.word_count = model_data['word_count']
            self.vocab_count = model_data['vocab_count']
            self.next_idx = model_data['next_idx']
            self.token_frequencies = model_data['token_frequencies']
            self.token_phases = model_data['token_phases']
            self.context_phases = model_data['context_phases']
            self.learn_rate = model_data['learn_rate']
            self.conversation_history = model_data['conversation_history']
            
            # Clear phase cache
            self.phase_cache = {}
            self.cache_hits = 0
            self.cache_misses = 0
            
            print(f"Model loaded from {filename}")
            print(f"  Vocabulary size: {self.vocab_count}/{self.vocab_size}")
            print(f"  Current learning rate: {self.learn_rate:.4f}")
            print(f"  Conversation history: {len(self.conversation_history)} tokens")
            return True
            
        except Exception as e:
            print(f"Error loading model: {e}")
            return False
    
    def visualize_token_space(self, tokens=None, n_tokens=20):
        """
        Visualize token phase space using dimensionality reduction.
        
        Parameters:
        -----------
        tokens : list or None
            List of specific tokens to visualize
        n_tokens : int
            Number of top tokens to visualize if tokens=None
        """
        # If no specific tokens, use most frequent
        if tokens is None:
            # Get most frequent tokens
            frequencies = [(i, self.token_frequencies[i]) for i in range(self.vocab_count)]
            frequencies.sort(key=lambda x: x[1], reverse=True)
            tokens = [i for i, _ in frequencies[:n_tokens]]
        
        # Get token phases
        phases = self.token_phases[tokens]
        
        # Convert to complex representation
        complex_rep = np.exp(1j * phases)
        
        # Use PCA-like approach to reduce to 2D
        # Just take first two dimensions for simplicity
        x = np.real(complex_rep[:, 0])
        y = np.imag(complex_rep[:, 0])
        
        # Plot
        plt.figure(figsize=(10, 8))
        plt.scatter(x, y, alpha=0.7)
        
        # Add token labels
        for i, token_idx in enumerate(tokens):
            plt.annotate(
                self.idx_to_word[token_idx],
                (x[i], y[i]),
                fontsize=9
            )
        
        plt.title("Token Phase Space Visualization")
        plt.xlabel("Re(φ₁)")
        plt.ylabel("Im(φ₁)")
        plt.grid(True, alpha=0.3)
        plt.axis('equal')
        plt.tight_layout()
        plt.show()
    
    def visualize_token_similarity(self, token, n_similar=10):
        """
        Visualize token similarities for a given token.
        
        Parameters:
        -----------
        token : str
            Token to visualize
        n_similar : int
            Number of most similar tokens to show
        """
        # Get token index
        if token in self.word_to_idx:
            token_idx = self.word_to_idx[token]
        else:
            print(f"Token '{token}' not found in vocabulary")
            return
        
        # Get token phase
        token_phase = self.token_phases[token_idx]
        
        # Calculate similarity to all other tokens
        similarities = []
        for i in range(self.vocab_count):
            if i == token_idx:
                continue
            
            similarity = self._phase_similarity(token_phase, self.token_phases[i])
            similarities.append((i, similarity))
        
        # Sort by similarity
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # Get top N similar tokens
        top_similar = similarities[:n_similar]
        
        # Plot
        plt.figure(figsize=(10, 6))
        tokens = [self.idx_to_word[i] for i, _ in top_similar]
        scores = [score for _, score in top_similar]
        
        plt.barh(tokens, scores)
        plt.title(f"Tokens Similar to '{token}'")
        plt.xlabel("Phase Similarity")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
    
    def export_vocabulary(self, filename="posalice_vocab.txt"):
        """
        Export vocabulary to a text file.
        
        Parameters:
        -----------
        filename : str
            Filename to export to
        """
        with open(filename, 'w', encoding='utf-8') as f:
            for word, count in sorted(self.word_count.items(), key=lambda x: x[1], reverse=True):
                f.write(f"{word}\t{count}\n")
        
        print(f"Vocabulary exported to {filename}")


class PosaliceChat:
    """
    Interactive chat interface for the Posalice Language Model.
    """
    
    def __init__(self, model=None, log_file=None):
        """
        Initialize the chat interface.
        
        Parameters:
        -----------
        model : PosaliceLanguageModel or None
            Language model instance
        log_file : str or None
            File to log conversation to
        """
        if model is None:
            self.model = PosaliceLanguageModel()
        else:
            self.model = model
        
        self.log_file = log_file
        self.conversation = []
        
        # Set up logging
        if log_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.log_file = f"posalice_chat_{timestamp}.txt"
        
        print(f"Chat session initialized. Logging to {self.log_file}")
    
    def _log_message(self, role, message):
        """Log a message to the conversation log."""
        with open(self.log_file, 'a', encoding='utf-8') as f:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"[{timestamp}] {role}: {message}\n")
        
        self.conversation.append((role, message))
    
    def start_chat(self):
        """Start the interactive chat session."""
        print("\n=== PosaliceChat: Unlimited Context Phase-Based Language Learning ===")
        print("Type your messages and the model will learn and respond.")
        print("Type 'exit', 'quit', or 'q' to end the session.")
        print("Type 'save' to save the current model.")
        print("Type 'load' to load a saved model.")
        print("Type 'vocab' to visualize the vocabulary.")
        print("Type 'similar <word>' to see similar words.")
        print("Type 'length=N' to set the maximum response length (default: 5000).")
        print("Type 'temp=N' to set the temperature (0.1-1.0, default: 0.7).")
        print("======================================================\n")
        
        running = True
        max_length = 5000
        temperature = 0.7
        
        while running:
            # Get user input
            user_message = input("> ")
            
            # Check for commands
            if user_message.lower() in ['exit', 'quit', 'q']:
                running = False
                continue
            
            elif user_message.lower() == 'save':
                filename = input("Enter filename to save model (default: posalice_model.pkl): ")
                if not filename:
                    filename = "posalice_model.pkl"
                self.model.save_model(filename)
                continue
            
            elif user_message.lower() == 'load':
                filename = input("Enter filename to load model (default: posalice_model.pkl): ")
                if not filename:
                    filename = "posalice_model.pkl"
                self.model.load_model(filename)
                continue
            
            elif user_message.lower() == 'vocab':
                n = input("Enter number of tokens to visualize (default: 20): ")
                try:
                    n = int(n) if n else 20
                except:
                    n = 20
                self.model.visualize_token_space(n_tokens=n)
                continue
            
            elif user_message.lower().startswith('similar '):
                word = user_message[8:].strip()
                self.model.visualize_token_similarity(word)
                continue
                
            elif user_message.lower().startswith('length='):
                try:
                    length = int(user_message[7:].strip())
                    max_length = max(10, length)
                    print(f"Maximum response length set to {max_length} tokens")
                    continue
                except:
                    print("Invalid length. Using default value.")
                    
            elif user_message.lower().startswith('temp='):
                try:
                    temp = float(user_message[5:].strip())
                    temperature = max(0.1, min(1.0, temp))
                    print(f"Temperature set to {temperature}")
                    continue
                except:
                    print("Invalid temperature. Using default value.")
            
            # Log user message
            self._log_message("User", user_message)
            
            # Show that the model is thinking for long generations
            if max_length > 100:
                print("Generating response...")
            
            # Get model response
            start_time = time.time()
            response = self.model.respond_to_message(user_message, max_length=max_length, temperature=temperature)
            
            # Calculate tokens per second
            elapsed = time.time() - start_time
            tokens = len(response.split())
            if elapsed > 0:
                speed = tokens / elapsed
                print(f"Generated {tokens} tokens in {elapsed:.2f} seconds ({speed:.1f} tokens/sec)")
            
            # Display and log model response
            print(f"AI: {response}")
            self._log_message("AI", response)
            
            # Adapt model
            self.model.adapt_to_conversation()
        
        print("\nChat session ended. Model has been trained on your inputs.")
        
        # Ask to save model
        save = input("Save model before exiting? (y/n): ")
        if save.lower() == 'y':
            filename = input("Enter filename (default: posalice_model.pkl): ")
            if not filename:
                filename = "posalice_model.pkl"
            self.model.save_model(filename)


# Example usage
if __name__ == "__main__":
    # Create language model with settings optimized for speed
    model = PosaliceLanguageModel(
        vocab_size=10000,  # Can handle a larger vocabulary
        embedding_dim=64,  # Reasonable dimension for balancing expressiveness and speed
        filter_param=0.3
    )
    
    # Create chat interface
    chat = PosaliceChat(model)
    
    # Start interactive chat
    chat.start_chat()
